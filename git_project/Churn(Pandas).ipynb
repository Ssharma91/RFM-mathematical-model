{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://www.thebluediamondgallery.com/handwriting/images/customer-satisfaction.jpg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHURN PREDICTION\n",
    "\n",
    "\n",
    "1. [Project Scope](#1)\n",
    "2. [Analytical approach](#1)\n",
    "    1. [Exploratory data Analysis](#1)\n",
    "        1. [Definition of training period , testing period and validation period](#1)\n",
    "        2. [Outlier removal](#1)\n",
    "        3. [Transposition of variables](#1)\n",
    "        4. [Missing value imputation](#1)\n",
    "    2. [Model Development](#1)\n",
    "        1. [Types of data pulled](#1)\n",
    "        2. [Treatment of CID level data](#1)\n",
    "        3. [Input data](#1)\n",
    "        4. [Category of variables used](#1)\n",
    "    3. [Code Snippets](#2)\n",
    "        1. [Logistic Regression](#2)\n",
    "        2. [Random Forest](#2)\n",
    "        3. [Gradient boosted machines](#2)\n",
    "        4. [Light GBM](#2)\n",
    "    4.[Selection of Algorithms (Basic -> Advanced)](#3)\n",
    "3. [Evaluation](#4)\n",
    "4. [Appendix](#4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Scope\n",
    "\n",
    "* To predict the likelihood of each customer to churn in the next 12 weeks future.\n",
    "* Identify key variables that influence customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical approach\n",
    "a. **Exploratory data Analysis**\n",
    "* Churn model built separately for **Online** and **Stores** customers.\n",
    "* Data divided into following time periods :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training data| Test data  | Out of time validation |\n",
    "| --- | --- | --- |\n",
    "| Sep-2017 to Aug-2018 | Nov-2018 - Oct 2019 | Sep-2016 to Aug-2017  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Treatment of customer level data \n",
    "    *  **Categorical importance** : Categorical variables transposed to get relevance of each category. Eg: contribution of male and female customers.\n",
    "    *  **Outlier removal** : Variables beyond 99th % capped with 99th % value.\n",
    "    *  **Missing value imputation** : Missing values were imputated keeping the initial variable composition intact\n",
    "    *  **Elimination of multicollinearity\n",
    "\n",
    "b. **Model Development**\n",
    "\n",
    "* **Types of Data pulled**\n",
    "    * Transaction data : Sales , trips , units , margin\n",
    "    * Demographic data : Ethnicity , Age , Gender\n",
    "    * Seasonal data : Holiday vs non holiday\n",
    "    * Loyalty data : credit card holders , loyalty customers\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GOLD</th>\n",
       "      <th>11</th>\n",
       "      <th>Credit</th>\n",
       "      <th>STORES</th>\n",
       "      <th>FEMALE</th>\n",
       "      <th>55-64</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>Children</th>\n",
       "      <th>$50K-75K</th>\n",
       "      <th>Single</th>\n",
       "      <th>...</th>\n",
       "      <th>1.19</th>\n",
       "      <th>0.27</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.29</th>\n",
       "      <th>1.20</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.32</th>\n",
       "      <th>1.21</th>\n",
       "      <th>1.22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLATINUM</td>\n",
       "      <td>14</td>\n",
       "      <td>NCL</td>\n",
       "      <td>STORES</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>25-34</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>No Children</td>\n",
       "      <td>$40K-50K</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOLD</td>\n",
       "      <td>16</td>\n",
       "      <td>Credit</td>\n",
       "      <td>STORES</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>35-44</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>No Children</td>\n",
       "      <td>&lt;$15K</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RED</td>\n",
       "      <td>19</td>\n",
       "      <td>NCL</td>\n",
       "      <td>STORES</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>18-24</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>No Children</td>\n",
       "      <td>&lt;$15K</td>\n",
       "      <td>Single</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RED</td>\n",
       "      <td>21</td>\n",
       "      <td>NCL</td>\n",
       "      <td>STORES</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>65-74</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Children</td>\n",
       "      <td>$50K-75K</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RED</td>\n",
       "      <td>22</td>\n",
       "      <td>NCL</td>\n",
       "      <td>STORES</td>\n",
       "      <td>FEMALE</td>\n",
       "      <td>45-54</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Children</td>\n",
       "      <td>$150K-$175K</td>\n",
       "      <td>Married</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GOLD  11  Credit  STORES  FEMALE  55-64   Hispanic     Children  \\\n",
       "0  PLATINUM  14     NCL  STORES  FEMALE  25-34   Hispanic  No Children   \n",
       "1      GOLD  16  Credit  STORES  FEMALE  35-44   Hispanic  No Children   \n",
       "2       RED  19     NCL  STORES  FEMALE  18-24   Hispanic  No Children   \n",
       "3       RED  21     NCL  STORES  FEMALE  65-74   Hispanic     Children   \n",
       "4       RED  22     NCL  STORES  FEMALE  45-54  Caucasian     Children   \n",
       "\n",
       "      $50K-75K   Single  ... 1.19  0.27  0.28  0.29  1.20  0.30  0.31  0.32  \\\n",
       "0     $40K-50K   Single  ...    0     0     0     1     0     0     0     0   \n",
       "1        <$15K   Single  ...    0     0     1     0     0     0     0     0   \n",
       "2        <$15K   Single  ...    0     0     1     0     0     0     0     0   \n",
       "3     $50K-75K  Married  ...    1     1     0     0     1     0     0     0   \n",
       "4  $150K-$175K  Married  ...    1     1     0     0     0     0     1     0   \n",
       "\n",
       "   1.21  1.22  \n",
       "0     1     1  \n",
       "1     1     1  \n",
       "2     1     1  \n",
       "3     1     1  \n",
       "4     1     1  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "from joblib import Parallel, delayed\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()  # the data has 78 variables to start with\n",
    "\n",
    "\n",
    "############ Split into train and test sets ####################\n",
    "\n",
    "feature = [x for x in train.columns if x not in 'Sales']\n",
    "y = train[['ID','Sales']]\n",
    "X = train[feature]\n",
    "\n",
    "X = X.set_index('ID')\n",
    "test = test.set_index('ID')\n",
    "y = y.set_index('ID')\n",
    "\n",
    "\n",
    "\n",
    "############# Remove multicollinearity  ##########################\n",
    "\n",
    "# Defining the function that you will run later\n",
    "def calculate_vif_(X, thresh=5.0):\n",
    "    variables = [X.columns[i] for i in range(X.shape[1])]\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        dropped=False\n",
    "        print(len(variables))\n",
    "        vif = Parallel(n_jobs=-1,verbose=5)(delayed(variance_inflation_factor)(X[variables].values, ix) for ix in range(len(variables)))\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print(time.ctime() + ' dropping \\'' + X[variables].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables.pop(maxloc)\n",
    "            dropped=True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print([variables])\n",
    "    return X[[i for i in variables]]\n",
    "\n",
    "feature_list = [x for x in train.columns ]\n",
    "df = train[feature_list] # Selecting your data\n",
    "\n",
    "df2 = calculate_vif_(df,5) # Actually running the function\n",
    "\n",
    "##################### Remove out;iers ###########################\n",
    "\n",
    "upper_lim = train['Sales'].quantile(.98)\n",
    "lower_lim = train['Sales'].quantile(.01)\n",
    "train.loc[(train['Sales'] > upper_lim),'Sales'] = upper_lim\n",
    "train.loc[(train['Sales'] < lower_lim),'Sales'] = lower_lim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Logistic regression in SAS\n",
    "proc logistic data=model.stores_2018 descending outest=output_sample outmodel=model.LogisticModel;\n",
    "Class &Cat_var./param=reference ref=first;\n",
    "model CHURN_FLAG(event='1') = &All_var./ \n",
    "CL selection=stepwise sle=.01 sls=.01 RSQUARE WALDCL LACKFIT CTABLE;\n",
    "      output out=model.predictions_2019 predicted=p_1 predprob=(individual crossvalidate);\n",
    "score data=model.stores_2019 out=model.validation_2019;\n",
    "run;\n",
    "\n",
    "\n",
    "\n",
    "### Random Forest \n",
    "rf = RandomForestClassifier(labelCol=\"CHURN_FLAG\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(trainingData)\n",
    "feature_importance = rf_model.featureImportances\n",
    "rf_prediction = rf_model.transform(testData)\n",
    "\n",
    "\n",
    "### GBM \n",
    "gbt = GBTClassifier(labelCol=\"CHURN_FLAG\",featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CHURN_FLAG\")\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "crossval = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "model = crossval.fit(trainingData)\n",
    "model.save(sc, \"gbt.model\")\n",
    "print model.featureImportances\n",
    "gbt_prediction = model.transform(testData) \n",
    "\n",
    "\n",
    "\n",
    "### Light GBM \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgbm = lgb.LGBMClassifier( n_estimators= 300,\n",
    "    colsample_bytree = 0.7,    max_depth= 20,    num_leaves= 100,    reg_alpha= 1.1,\n",
    "    reg_lambda= 1.1,    min_split_gain= 0.4,    subsample=0.8,    subsample_freq =20)\n",
    "model = lgbm.fit(X_train, y_train,\n",
    "         eval_set=[(X_test, y_test)],\n",
    "        eval_metric='auc',\n",
    "          early_stopping_rounds=5  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Categorization for identifying customers at risk :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Score |Risk | \n",
    "| --- | --- | \n",
    "| >.72 | High probability to churn | \n",
    "| .56 - .74 |Equally likely to churn|\n",
    "| .4-.56 | Least likely to churn| \n",
    "| <.44 | Minimal risk customers |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of algorithm (Basic -> advanced)\n",
    "* Exploratory data analysis was performed on the data using PySpark and Python Scripts\n",
    "* Multiple algorithms were used to predict which customers are likely to churn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Metrices |Logistic Regression | Random Forest | Gradient boosted machine |Light GBM |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| centered Platform used | SAS | Pyspark/Pandas | Pyspark/Pandas | Pandas|\n",
    "| Variable importance | varaibles with iv value < .02 rejected | All|All| All|\n",
    "| Multicollinearity Check | eliminated (VIF) | not required | not required |not required |\n",
    "| Final variable selection | ~69 | ~128 | ~128 | ~128 |\n",
    "| Accuracy | Online(76%) & Stores(73%) | 76% (20% data) | 86%(20% data) | 80%(Store data)|\n",
    "| Recall | 76 | 76 |76|76|\n",
    "| Caveat | Time consuming and accuarcy < 80% | No significant improvisation over Logistic Regression| Takes 3x time |Results need optimization|\n",
    "| Staus | Accepted as baseline model | Rejected|Rejected|In progress|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation : Comparison of Light GBM with Logistic Regression for Store data\n",
    "### In-time validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics | Logistic Regressisom  | Light GBM|\n",
    "| --- | --- | --- |\n",
    "| Accuracy |70% | 80%|\n",
    "| AUC | 73% | 86%|\n",
    "| Recall | 72% | 77% |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Terminologies and Algorithm Description\n",
    "**Logistic regression:**\n",
    "*Weight of Evidence and information values calculated for all variables. Variables with iv score < .2 excluded.\n",
    "*Multicollinearity : Variables with VIF > 5 excluded\n",
    "*Logistic regression can predict probabilities restricted between 0 and 1 of a customer getting churned and the coefficients of the model also provide a hint of the relative importance of each input variable.\n",
    "\n",
    "**Random Forest**\n",
    "*It consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our modelâ€™s prediction\n",
    "*They ensure that the behavior of each individual tree is not too correlated\n",
    "*Bagging (Bootstrap Aggregation) : Random forest takes advantage of this by allowing each individual tree to randomly sample from the dataset with replacement, resulting in different trees. \n",
    "*Feature Randomness â€” In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. In contrast, each tree in a random forest can pick only from a random subset of features. \n",
    "*Feature importance can be extracted and the relevant features can be fed one by one to measure the model performance.\n",
    "\n",
    "**Gradient boosted Trees:**\n",
    "*Machine learning boosting is a method for creating an ensemble. It starts by fitting an initial model (e.g. a tree or linear regression) to the data. Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly. The combination of these two models is expected to be better than either model alone. Then you repeat this process of boosting many times.  Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models.\n",
    "*Feature importance can be extracted and the relevant features can be fed one by one to measure the model performance.\n",
    "\n",
    "**Light GBM :**\n",
    "*Light GBM is a gradient boosting framework that uses tree based learning algorithm.\n",
    "*Light GBM is prefixed as â€˜Lightâ€™ because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results.\n",
    "\n",
    "**Confusion matrix and recall**\n",
    "(https://www.researchgate.net/figure/Modified-Confusion-Matrix-Table-for-Accuracy-Prediction-of-24_fig4_319183193)\n",
    "**ROC & AUC** (https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
