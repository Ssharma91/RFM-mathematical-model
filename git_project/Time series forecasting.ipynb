{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME SERIES FORCASTING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Project Scope](#1)\n",
    "2. [What is Time series?](#1)\n",
    "3. [Components of Time series forecasting](#2)\n",
    "4. [Let’s step back  : Basics of modeling a time series data](#3)\n",
    "5. [How to approach a forecasting problem](#4)   \n",
    "6. [Foundation of popular algorithms like ARIMA / SARIMA](#5)\n",
    "7. [Getting the data](#6)\n",
    "8. [Forecasting using SARIMAX and LSTM ](#7)\n",
    "9. [Evaluation](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Scope\n",
    "\n",
    "1. **Objective** : To segment customers as per their shopping patterns and predict future footfalls.\n",
    "2. **What** : Forecast number of trips at different levels across stores/online identify patterns\n",
    "3. **Why**: To estimate futuristic trips and drive traffic\n",
    "\n",
    "    \n",
    "## What is time series \n",
    "\n",
    "Time Series Modeling. As the name suggests, it involves working on time (years, days, hours, minutes) based data, to derive hidden insights to make informed decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components of Time series forecasting\n",
    "\n",
    "1. **Trend**: Series could be constantly increasing or decreasing or first decreasing for a considerable period and then decreasing. \n",
    "    his trend is identified and then removed from the time series in ARIMA forecasting process.\n",
    "\n",
    "2. **Seasonality** : Repeating pattern with fixed period.\n",
    "Example - Sales in festive seasons. Sales of Candies and sales of Chocolates peaks in every October Month and \n",
    "December month respectively every year in US. It is because of Halloween and Christmas falling in those months. \n",
    "The time-series should be de-seasonalized in ARIMA forecasting process.\n",
    "\n",
    "3. **Random Variation** (Irregular Component)\n",
    "This is the unexplained variation in the time-series which is totally random.\n",
    "Erratic movements that are not predictable because they do not follow a pattern. Example – Earthquake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pic4.jpg\" align = \"left\" height = \"300\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pandas as pd\n",
    "df = pd.read_csv('master_data.csv')\n",
    "df.head()\n",
    "y = df[['trips']]\n",
    "feature = ['Holiday_Flag', 'Promotion_Flag']\n",
    "result = seasonal_decompose(y, model='additive',freq = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics – Time Series Modeling\n",
    "\n",
    "a. **Stationary Series**: stationary series is one whose mean and variance of the series is constant over time. \n",
    "To calculate the expected value, we generally take a mean across time intervals. \n",
    "The mean across many time intervals makes sense only when the expected value is the same across those time periods. \n",
    "If the mean and population variance can vary, there is no point estimating by taking an average across time. \n",
    "In cases where the stationary criterion are violated, the first requisite becomes to stationaries’ \n",
    "the time series and then try stochastic models to predict this time series.\n",
    "There are multiple ways of bringing this stationarity. Some of them are Detrending, Differencing etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"stationary.jpg\" align = \"left\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trips'].plot(x='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. **Stationarity Test** : Augmented Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATIONARITY TEST\n",
    "\n",
    "series = df.loc[:, 'trips'].values\n",
    "result = adfuller(series, autolag='AIC')\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'n_lags: {result[1]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "for key, value in result[4].items():\n",
    "    print('Critial Values:')\n",
    "    print(f'   {key}, {value}')    \n",
    "    \n",
    "#The p-value is obtained is greater than significance level of 0.05 and the ADF statistic is higher than any of\n",
    "#the critical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. **Eliminate Trend and seasonality**\n",
    "1. Differencing – taking the difference with a particular time lag\n",
    "2. Decomposition – modeling both trend and seasonality and removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to approach Time series problem:\n",
    "1. Naïve Approach        \n",
    "2. Simple Average \n",
    "3. Moving average \n",
    "4. Simple Exponential Smoothing \n",
    "5. Holt’s Linear Trend Method  \n",
    "6. Holt Filter\n",
    "7. ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pic1.jpg\" align = \"left\" height = \"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"pic2.jpg\" align = \"left\" height= \"400\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARIMA in detail**\n",
    "\n",
    "**Auto-regressive component**\n",
    "It implies relationship of a value of a series at a point of time with its own previous values. Such relationship can exist with any order of lag.\n",
    "Lag - Lag is basically value at a previous point of time. It can have various orders as shown in the table below. It hints toward a pointed relationship.\n",
    "\n",
    "**Moving average components**\n",
    "It implies the current deviation from mean depends on previous deviations. Such relationship can exist with any number of lags which decides the order of moving average.\n",
    "Moving Average - Moving Average is average of consecutive values at various time periods.  It can have various orders as shown in the table below. It hints toward a distributed relationship as moving itself is derivative of various lags.\n",
    "\n",
    "                        \n",
    "**Difference between AR and MA models**\n",
    "The primary difference between an AR and MA model is based on the correlation between time series objects at different time points. The correlation between x(t) and x(t-n) for n > order of MA is always zero. This directly flows from the fact that covariance between x(t) and x(t-n) is zero for MA models (something which we refer from the example taken in the previous section). However, the correlation of x(t) and x(t-n) gradually declines with n becoming larger in the AR model. This difference gets exploited irrespective of having the AR model or MA model. The correlation plot can give us the order of MA model.\n",
    "\n",
    "\n",
    "**What order of AR or MA process do we need to use?**\n",
    "\n",
    "The first question can be answered using Total Correlation Chart (also known as Auto – correlation Function / ACF). ACF is a plot of total correlation between different lag functions. For instance, in GDP problem, the GDP at time point t is x(t). We are interested in the correlation of x(t) with x(t-1) , x(t-2) and so on. Now let’s reflect on what we have learnt above.\n",
    "In a moving average series of lag n, we will not get any correlation between x(t) and x(t – n -1) . Hence, the total correlation chart cuts off at nth lag. So, it becomes simple to find the lag for a MA series. For an AR series, this correlation will gradually go down without any cut off value. So what do we do if it is an AR series?\n",
    "Here is the second trick. If we find out the partial correlation of each lag, it will cut off after the degree of AR series. For instance, if we have a AR(1) series,  if we exclude the effect of 1st lag (x (t-1) ), our 2nd lag (x (t-2) ) is independent of x(t). Hence, the partial correlation function (PACF) will drop sharply after the 1st lag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introducing SARIMAX**\n",
    "When the ARIMA model is further applied to the seasonal part, we get Seasonal ARIMA. ARIMA generates p,d,q values for \n",
    "stationary data, SARIMAX produces p,d,q  values for the seasonal part separately as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting code using Sarimax and Lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "stepwise_model = auto_arima(df['trips'],exogenous= X[['Holiday_Flag', 'Promotion_Flag']]\n",
    "                            ,start_p=0, start_q=0,\n",
    "                         max_p=5, max_q=5, m=1,\n",
    "                        start_P=0, seasonal=True,\n",
    "                           d=0, D=1, trace=True,\n",
    "                           error_action='ignore',  \n",
    "                           suppress_warnings=True, \n",
    "                           stepwise=True)\n",
    "stepwise_model.summary()  \n",
    "\n",
    "#Train Test Split\n",
    "train = df[0:776]\n",
    "X_train = train[feature]\n",
    "y_train = train[[target]]\n",
    "\n",
    "test = df[776:779]\n",
    "X_test = test[feature]\n",
    "y_test = test[[target]]\n",
    "\n",
    "\n",
    "# Get the order values from stepwise_model.summary()\n",
    "model = SARIMAX(y_train[[target]], exogenous = X_train[feature],\n",
    "                order = (4, 0, 1),  \n",
    "                seasonal_order =(0, 0, 0, 1)) \n",
    "  \n",
    "result = model.fit() \n",
    "\n",
    "\n",
    "start = len(train) \n",
    "end = len(train) + len(test) - 1 \n",
    "\n",
    "predictions  = result.predict(start,end,return_conf_int=False).rename(\"Predictions\") \n",
    "\n",
    "\n",
    "#Performance measurement\n",
    "output = pd.concat([test, predictions], axis=1)\n",
    "output.head()\n",
    "output['mape'] = abs(output['Predictions']-output['dmd_dol'])/output['dmd_dol']\n",
    "mape = output['mape']\n",
    "mape.mean() * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM  using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def to_supervised(df,dropNa = True,lag=1):\n",
    "    #df = pd.DataFrame(data)\n",
    "        column = []\n",
    "        column.append(df)\n",
    "        for i in range(1,lag+1):\n",
    "            column.append(df.shift(-i))  #lags added here\n",
    "        df = pd.concat(column,axis=1)\n",
    "        df.dropna(inplace = True)\n",
    "        features = merged.shape[1]\n",
    "        df = df.values\n",
    "        supervised_data = df[:,:features*lag]\n",
    "        supervised_data = np.column_stack( [supervised_data, df[:,features*lag]])\n",
    "        return supervised_data\n",
    "\n",
    "    supervised = to_supervised(merged,lag=timeSteps)\n",
    "    dataframe = pd.DataFrame(supervised)\n",
    "    \n",
    "    \n",
    "    df = dataframe.rename({0: 'trips',  1:'Promotion_Flag',2:'Holiday_flag' 2:'lag_trips'}, axis=1)\n",
    "    ## Notice the lag trips, try to visualize the dataframe first.\n",
    "\n",
    "\n",
    "    cols_to_scale = ['trips','lag_trips']\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_cols = scaler.fit_transform(df[cols_to_scale])\n",
    "    df[cols_to_scale] = scaled_cols\n",
    "\n",
    "    X = df.drop(['lag_trips'], axis=1)\n",
    "    Y = df['lag_trips']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.205, shuffle=False)\n",
    "    #return X_test\n",
    "\n",
    "#Rest col : check if rest columns calculated correctly\n",
    "\n",
    "    rest_col=rest_col[['cm','gross_amt']]\n",
    "    dummy=rest_col['cm']\n",
    "    rest_train, rest_test, dummy_train, dummy_test = train_test_split(rest_col,dummy , test_size=0.205, shuffle=False)\n",
    "    #return rest_test\n",
    "    rest_test.index = range(27) \n",
    "  \n",
    "    \n",
    "    \n",
    "    X_train_vals = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test_vals = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    def build_model():\n",
    "        model = Sequential()    \n",
    "        model.add(LSTM(50, input_shape = (X_train_vals.shape[1], X_train_vals.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mape'])\n",
    "        return model\n",
    "\n",
    "    model = build_model()\n",
    "\n",
    "    history = model.fit(X_train_vals, y_train.values,  epochs=200, \n",
    "    batch_size=1,validation_data=(X_test_vals, y_test.values),verbose=2,\n",
    "    shuffle=False)\n",
    "    \n",
    "    y_pred = model.predict(X_test_vals)\n",
    "    #return y_pred\n",
    "    X_test_vals = X_test_vals.reshape(X_test_vals.shape[0],X_test_vals.shape[2]*X_test_vals.shape[1])\n",
    "    \n",
    "    inv_new = np.concatenate( (y_pred, X_test_vals) , axis =1) \n",
    "    return inv_new\n",
    "    inv_new = inv_new[:,0:2]\n",
    "    inv_new = scaler.inverse_transform(inv_new)\n",
    "    output = inv_new\n",
    "    output = np.delete(output,0,axis=0)\n",
    "    #rest_test.drop(rest_test.index[0])  remove 1st row\n",
    " \n",
    "      \n",
    "\n",
    "    #output = inv_new\n",
    "    output_df = pd.DataFrame({'predicted':output[:,0],'actual':output[:,1]})\n",
    "    output_df = pd.concat([output_df, rest_test], axis=1)\n",
    "    output_df['mape'] = abs(output_df['predicted']-output_df['actual'])/output_df['actual']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance (Basic to advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Metrices | Seasonal ARIMAX | LSTM |\n",
    "| --- | --- | --- |\n",
    "| Accuracy | 90% | 86% | \n",
    "| Caveat | Time consuming |Faster than ARIMA but requires high GPU|\n",
    "| Status | Accepted as baseline model | Accepted|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Bonus Tip:</b> For LSTM to fucntion, you would need to manually decide the timesteps for which you want LSTM\n",
    "    to remember the informaion, if the data is for 52 weeks, you might want to just look at previous 3 weeks to \n",
    "    predict the future data and create the lag variable accrodingly. The whole point of LSTM is to create a lag \n",
    "    variable smartly,domain knowledge plays a very important role here </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
